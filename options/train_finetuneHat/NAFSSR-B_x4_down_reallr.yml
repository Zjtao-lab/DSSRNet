# ------------------------------------------------------------------------
# Copyright (c) 2022 megvii-model. All Rights Reserved.
# ------------------------------------------------------------------------
# Modified from BasicSR (https://github.com/xinntao/BasicSR)
# Copyright 2018-2020 BasicSR Authors
# ------------------------------------------------------------------------
# general settings
name: NAFNetSR-B_x4_simpleffb_finetuneHatSR_down__withReallr_v0.1
model_type: ImageRestorationModel
scale: 4
num_gpu: 2
manual_seed: 10

datasets:
  train:
    name: Flickr1024-sr-train
    type: hatPassPairedStereoImageDataset  # PairedStereoImageDataset
    dataroot_gt: /home/thinkstation03/zjt/NAFNet-data/train/finetuneHAT-L_sisr_withreallr_patches_x4/
    dataroot_lq: /home/thinkstation03/zjt/NAFNet-data/train/finetuneHAT-L_sisr_withreallr_patches_x4/
    dataroot_reallq: /home/thinkstation03/zjt/NAFNet-data/train/finetuneHAT-L_sisr_withreallr_patches_x4/
    io_backend:
      type: disk

    gt_size_h: 120
    gt_size_w: 360
    use_hflip: true
    use_vflip: true
    use_rot: false
    flip_RGB: true
    # mint , mixup, randomtrans

    # data loader
    use_shuffle: true
    num_worker_per_gpu: 4
    batch_size_per_gpu: 2 # before 4
    dataset_enlarge_ratio: 1
    prefetch_mode: ~

  val:
    name: Flickr1024-sr-test
    type: hatPassPairedStereoImageDataset  # PairedStereoImageDataset
    dataroot_gt: /home/thinkstation03/zjt/NAFNet-data/test/Flickr1024/hr
    dataroot_lq: /home/thinkstation03/zjt/NAFNet-data/test_finetuneHAT-L/Flickr1024/lr_x4
    dataroot_reallq: /home/thinkstation03/zjt/NAFNet-data/test_finetuneHAT-L/Flickr1024/lr_x4
    io_backend:
      type: disk

# network structures
network_g:
  type: NewsimpleFFCwithHAT_down_withCSAM_SSR
  # NewsimpleFFCwithHAT_down_SSR
    # NewsimpleFFCwithHATSSR
  up_scale: 4
  width: 96 # 64
  num_blks: 36  # B:42 S:28 fault 32
  drop_path_rate: 0.1 # fault 0.1
  train_size: [1, 12, 120, 360]
  drop_out_rate: 0.

# 加载hat时候用到的参数, may used
hat_net_root:
  # hat项目的路径 和 配置文件的路径
  # 这里是为了得到加载训练参数后的hat-net
  root_path: "/home/thinkstation03/zjt/HAT/hat_git"
  opt_path: "/home/thinkstation03/zjt/HAT/hat_git/options/testA5k/HAT-L_SRx4_ImageNet-finetune.yml"


# path
path:
  pretrain_network_g: /home/thinkstation03/zjt/NAFNet-ALL/experiments/NAFNetSR-B_x4_simpleffb_finetuneHatSR_down__withReallr_v0.1_g/models/net_g_10000.pth # 不用的就设为：~
  strict_load_g: false
  resume_state:  ~  # 需要的话也是填路径
  save_info_outer_main: true

# training settings
train:
  optim_g:
    type: AdamW
    lr: !!float 3e-3   # 3e-3   --> next try 1e-3
    weight_decay: !!float 0
    betas: [0.9, 0.9]

  scheduler:
    type: TrueCosineAnnealingLR
    T_max: 70000   # v1: 70000
    eta_min: !!float 1e-6  # fault 1e-7

  total_iter: 70000  # v1: 70000
  warmup_iter: -1 # no warm up
  mixup: false

  # losses
  pixel_opt:
    type: MSELoss
    loss_weight: 1.
    reduction: mean

  # finetune
  finetune_byjintao:
    flag: false # or false
    need_keys: ['body.30']  # 这里记得灵活改写

# validation settings
val:
  val_freq: !!float 5e3
  save_img: false
  trans_num: 1

  max_minibatch: 1

  metrics:
    psnr: # metric name, can be arbitrary
      type: calculate_psnr
      crop_border: 0
      test_y_channel: false
    ssim:
      type: calculate_skimage_ssim

# logging settings
logger:
  print_freq: 200
  save_checkpoint_freq: !!float 5e3
  use_tb_logger: true
  wandb:
    project: ~
    resume_id: ~

# dist training settings
dist_params:
  backend: nccl
  port: 29500
